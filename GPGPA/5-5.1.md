# 5 关于 GPU 计算架构的跨领域研究

本章详细介绍了几个在 GPGPU 架构方面的研究方向，这些方向并不完全适合于前面几章中对 GPU 架构特定部分的讨论。第 5.1 节探讨了有关 GPU 中线程调度的工作。第 5.2 节研究了替代编程方法，第 5.4 节则审视了异构 CPU/GPU 计算方面的研究工作。

# 5.1 线程调度

现代GPU在本质上与CPU有根本区别，GPU依赖于大规模的并行处理。无论程序是如何定义的（例如，使用OpenCL、CUDA、OpenACC等），缺乏广泛软件定义并行性的工作负载都不适合进行GPU加速。GPU采用几种机制来聚合和调度所有这些线程。GPU上线程的组织和调度主要有三种方式。

**线程分配到 warp **：由于GPU使用SIMD单元执行由MIMD编程模型定义的线程，因此线程必须以 warp 的形式结合在一起以进行锁步执行。在本书研究的基础GPU架构中，具有连续线程ID的线程被静态结合在一起以形成 warp 。3.4.1节总结了关于 warp 内替代线程排列以实现更好 warp 紧凑性的研究建议。

**线程块动态分配给核心**：与CPU中线程可以逐个分配给硬件线程的方式不同，在GPU中，工作是批量分配给GPU核心的。这项工作单位由多个以线程块形式存在的波组成。在我们的基准GPU中，线程块以轮询方式分配给核心。核心的资源（如波槽、寄存器文件和共享内存空间）以线程块为粒度进行划分。由于与每个线程块关联的大量状态，当代GPU并不预empt执行。线程块中的线程会运行到完成，然后其资源才会分配给另一个线程块。

**周期性调度决策**：在一个线程块被分配给GPU核心后，一组精细粒度的硬件调度器在每个周期决定哪些warp集合获取指令，哪些warp发出指令供执行，以及何时读取/写入每个发出指令的操作数。

**调度多个内核**：线程块级和逐周期的调度决策可以在一个内核内以及在同一GPU上并发运行的不同内核之间进行。传统的内核调度限制了每次只能在GPU上激活一个内核。然而，NVIDIA的流和HyperQ调度机制的引入使得并发内核的运行成为可能。在某些方面，这种情况类似于CPU上的多程序运行。

## 5.1.1 线程块到核心的分配研究

当内核启动时，组成该内核的线程被分组成线程块。一种 GPU 范围的线程块调度机制根据资源可用性将每个线程块分配给一个 SIMT 核心。在我们研究的基线 GPU 中，线程块以轮询方式分配给核心。核心的资源（如 warp 槽、寄存器文件和共享内存空间）以线程块为单位进行预留。由于每个核心预留的线程块资源数量有限，只要核心的至少一种资源被耗尽，线程块的分配就会停止。请注意，一个内核可能由比 GPU 一次能运行的线程块更多的线程块组成。因此，内核的一些线程块可能不会在 GPU 上运行，而其他线程块则在执行。几种技术研究了线程块调度的空间权衡。

**在核心级别限制线程块数量**：Kayiran 等人 [2013] 提出了通过限制分配给每个核心的线程块数量来减少因线程过度订阅而导致的内存系统中的争用。他们开发了一种算法，监控核心空闲周期和内存延迟周期。该算法从将每个核心的线程块数量限制为最大线程块数量的一半开始。然后监控空闲和内存延迟周期。如果核心主要在等待内存，则不再向其分配更多的线程块，并且现有的线程块可能会被暂停以停止发出指令。该技术通过限制活跃的线程块数量，实现了粗粒度的并行性限制机制，从而减少了内存系统的干扰并提高了整体应用性能，即使活跃的线程块数量减少。

**动态调整 GPU 资源**：Sethia 和 Mahlke [2014] 提出了 Equalizer，一种硬件运行时系统，它动态监控资源争用并调整线程数量、核心频率和内存频率以提高能源效率和性能。该系统基于四个参数做出决策：（1）每个 SIMT 核心上的活跃 warp 数量；（2）等待内存数据的 warp 数量；（3）准备执行算术指令的 warp 数量；以及（4）准备执行内存指令的 warp 数量。基于这些参数，它首先决定每个 SIMT 核心上保持活跃的 warp 数量，然后基于此值和内存争用的代理值（计算密集度和内存密集度）决定如何最佳地调整核心和内存系统的频率。

 Equalizer 具有两种操作模式：节能模式和性能增强模式。在节能模式下，它通过缩减过度使用的资源来节省能源，同时尽量减少对性能的影响。在性能增强模式下，Equalizer 提高瓶颈资源，以增加性能并以能源效率的方式进行。

他们根据工作负载是计算密集型、内存密集型、缓存敏感型还是未饱和型来表征一组工作负载。如果最小化能源消耗（而不牺牲性能）是目标，那么计算密集型内核应在较低的内存频率下运行，而内存密集型内核应在较低的 SIMT 核心频率下运行。这有助于减少系统中未被充分利用的资源的能源消耗。

 Equalizer 基于本地决策在每个 SIMT 核心上做出决策。它在每个 SIMT 核心中添加监控硬件，以基于四个计数器做出本地决策。它决定本地在每个 SIMT 核心上应该使用多少个线程块（CTA），发出新的线程块或暂停核心上的线程块。在决定每个 SIMT 核心上的活动线程块数量后，如果 SIMT 核心需要更多的工作，它会向全局工作分配引擎请求新的线程块。如果 SIMT 核心应该减少其活动线程块数量，它会暂停核心上的一些线程块。在做出 SIMT 核心的线程块使用数量决策后，每个 SIMT 核心会提交内存/计算电压目标给全局频率管理器，全局频率管理器根据多数规则设置芯片范围的频率。

本地决策是通过观察等待执行内存指令的线程束数量和等待执行 ALU 指令的线程束数量来做出的。如果试图等待内存的线程束数量大于一个 CTA 中的线程束数量，则在该 SIMT 核心上运行的 CTA 数量会减少，从而可能有助于缓存敏感工作负载的性能。如果准备发出内存（或 ALU）指令的线程束数量超过一个 CTA 中的线程束数量，则该 SIMT 核心被视为内存（或计算）密集型。如果等待内存（或计算）的线程束数量少于一个 CTA 中的线程束数量，并且活跃线程束中有超过一半等待且等待内存的线程束不超过两个，则该 SIMT 核心仍然可以被视为 ALU 或内存绑定。如果是这种情况，则核心上的活跃 CTA 数量增加一个，并根据等待计算的线程束或等待内存的线程束的数量来判断该 SIMT 核心是计算密集型还是内存密集型。

一旦SIMT核心做出了本地决策，内存和核心的频率将根据均衡器正在操作的模式调整±15%。

## 5.1.2 每周期调度决策研究

**早期关于每周期调度决策的表征**：Lakshminarayana 和 Kim [2010] 在早期没有硬件管理缓存的 GPU 上探索了多种 warp 调度策略，并表明对于执行每个 warp 的动态指令计数对称（平衡）的早期 GPU 来说，基于公平的 warp 和 DRAM 访问调度策略可以提高性能。这种策略在他们研究的早期规则工作负载上表现出色，因为规则的内存请求可以在核心内合并，并且更好地利用 DRAM 行缓冲区局部性。该论文还表征了几种其他 warp 调度策略，包括 ICOUNT，该策略由 Tullsen 等人 [1996] 首次提出，用于同时多线程 CPU。ICOUNT 设计用于通过优先级最高的 warp（或线程）来提高系统吞吐量。Lakshminarayana 和 Kim [2010] 表明，在他们研究的早期缓存前的 GPU 上，优先级较高的 warp 调度策略通常不会改善性能。

**两层调度**：Gebhart 等人 [2011c] 引入了两层调度器，以提高能源效率。他们的两层调度器将核心中的 warp 分为两个池：一个活跃池，包含每周期考虑调度的 warp；以及一个非活跃池，包含不考虑调度的 warp。当 warp 遇到编译器识别的全局或纹理内存依赖项时，它会从活跃池转移到非活跃池，并以轮询方式从非活跃池中选择 warp。从每周期考虑较小的 warp 池可以减少 warp 选择逻辑的大小和能源消耗。

Narasiman 等人 [2011] 提出的两层调度关注于通过允许线程组在不同时间到达同一个长延迟操作来提高性能。这有助于确保缓存和行缓冲区局部性在取指组内得以维持。与缓存感知的 warp 调度（见下文）相比，它关注于通过自适应限制系统可以维持的多线程数量来提高性能，基于内存系统反馈。

**缓存感知的 warp 调度**：Rogers 等人 [2012] 将 GPU 内核中的内存局部性分类为 warp 内部局部性，其中 warp 加载然后引用其自己的数据，或 warp 间局部性，其中 warp 与其它 warp 共享数据。他们表明，缓存敏感的工作负载中最常见的局部性形式是 warp 内部局部性。基于此观察，他们提出了缓存感知的波浪调度（CCWS）机制，通过基于内存系统反馈限制活跃调度的 warp 数量来利用这种局部性。

积极地在较少的 warp 上调度可以为每个 warp 提供更多的缓存空间，并减少 L1 数据缓存争用。特别是，当具有局部性的 workload 缓存 thrashing 时，会触发限制。为了反映这种局部性损失，CCWS 引入了一种基于 L1 数据缓存替换的牺牲标签的局部性损失检测机制。

图5.1绘制了CCWS的高层微架构。在每次缓存驱逐时，牺牲标签被写入每个 warp 的专用牺牲标签数组。每个 warp 都有自己的牺牲标签数组，因为 CCWS 只关心检测 warp 内部局部性。在每次后续缓存未命中时，将探测缺失 warp 的牺牲标签数组。如果找到该标签，则表明某些 warp 内部局部性已经丢失。CCWS 做出假设，即这个 warp 可能会从更多的独占 L1 数据缓存访问中受益，因此可以限制。

通过发送信号到调度系统，局部性损失的分数被增加。该发行调度器使用局部性评分系统来近似估算系统中每个 warp 已经失去的局部性数量，这是每个 warp 需要的额外缓存容量的近似值。所有 warp 在局部性评分系统中都被分配了一个初始分数，假设所有 warp 需要相同的缓存容量，并且不进行限制（即，所有 warp 都可以发行）。随着时间的推移，如果检测到局部性损失，单个 warp 的分数就会增加。在图 5.1 的示例中，warp 0 已经历局部性损失，其分数已增加。这个增加的分数将推动 warp 3 的分数超过阈值，从而防止它发行 L1 数据缓存请求，有效地限制了在核心上积极调度的 warp 数量。随着时间的推移，如果没有局部性损失，warp 0 的分数就会减少，warp 2 就可以再次发行内存请求。

CCWS 进一步展示了缓存命中率对调度决策的敏感性，通过比较各种调度机制和缓存替换策略。该论文展示了 warp 调度器的决策空间远大于替换策略相对受限的决策空间。该论文还展示了 CCWS 方案使用 LRU 替换策略，即使在具有 Belady 最优缓存替换策略的情况下，也能比之前的调度机制更有效地提高缓存命中率。

Rogers 等人 [2013] 提出了 Divergence-Aware Warp Scheduling（DAWS），它通过扩展 CCWS 来更准确地估计每个 warp 的缓存占用。DAWS 利用大多数 GPU 工作负载中 warp 内部局部性发生在循环中的事实。DAWS 创建了一个 per-warp 缓存占用估计，用于循环中的 warp。DAWS 根据每个 warp 经历的控制流发散程度，预先限制循环中的 warp 数量。已经离开循环的 warp 的线程不再对占用估计做出贡献。DAWS 进一步探索了 GPU 的可编程性，展示了通过更智能的 warp 调度器，一个没有对内存传输进行优化的基准测试（例如，使用共享内存而不是缓存）可以接近与经过优化的 GPU 版本相媲美的性能。

**预取感知的 warp 调度**：Jog 等人 [2013b] 探索了基于两级调度机制的预取感知 warp 调度器。他们基于两级调度机制构建了预取组，但通过非连续的 warp 形成预取组。此策略增加了 DRAM 中的 bank 级并行性，因为一个 DRAM bank 不会连续地被预取器查询。他们还扩展了此想法，根据 warp 组的分配来操纵预取器。通过为其他组的 warp 预取数据，他们可以改善行缓冲区局部性，并为落后于预取请求的 warp 提供数据间隔。

**CTA 感知调度**：Jog 等人 [2013a] 提出了基于两级调度的 CTA 感知 warp 调度器，以选择性地组合 CTA。他们利用几个基于 CTA 的属性来提高性能。除了类似于并发工作的 warp 调度限制外，他们还利用了不同核心上的 CTA 的跨 CTA 页面局部性。在仅考虑局部性的 CTA 调度中，连续的 CTA 通常会同时访问同一个 DRAM bank，从而减少 bank 级并行性。他们结合了预取机制，也在不同的核心之间预取数据，以改善 DRAM 行缓冲区局部性。

**调度对分支发散缓解技术的影响**：Meng 等人 [2010] 引入了 Dynamic Warp Subdivision（DWS），当一些线程在缓存中命中而其他线程未命中时，它会分割 warp。这种方案允许缓存命中的线程在其他线程的 warp 同伴未命中时继续前进，从而提前启动其未命中的线程，产生预取效果。

Fung 等人 [2007] 探索了 warp 调度策略对 Dynamic Warp Formation（DWF）的有效性的影响。DWF 尝试通过在分支指令上动态创建新 warp 来缓解控制流发散。他们提出了五种调度器，并评估了它们对 DWF 的影响。

Fung 和 Aamodt [2011] 还提出了三种线程块优先机制，以补充他们的 Thread Block Compaction（TBC）技术。这些优先机制试图一起调度同一个 CTA 中的线程。他们的方法类似于 Narasiman 等人 [2011] 提出的并发工作取指组的两级调度，但线程块被一起调度而不是取指组。

第 3.4 节包含有关 DWS、DWF 和 TBC 的更详细摘要。

**调度和缓存重发**：Sethia 等人 [2015] 提出了 Mascar，它试图更好地将计算与 GPU 上的内存访问重叠。Mascar 包括两个相互关联的机制。

- 一个内存感知的 warp 调度器（MAS），它优先执行单个 warp，当 MSHR 和 L1 错过队列在核心中被过度订阅时。这种优先级有助于即使在工作负载没有数据局部性时也能提高性能，使 warp 在 L1 中的计算操作能够更快地完成，从而允许与其他 warp 的内存访问重叠。
- 一个缓存访问重发（CAR）机制，它通过提供 L1 数据缓存的命中下缺失功能，帮助避免 L1 数据缓存 thrashing，即使 warp 持有缓存中的数据但由于 LSU 堵塞而无法发出。

MAS 有两种作模式：同等优先级 （EP） 和内存访问优先级 （MAP）模式。系统根据 L1 MSHR 和内存未命中队列的满度在 EP 和 MP 之间切换。一旦这些结构几乎装满，系统就会切换到 MP 模式。MAS 包含两个队列，一个用于 Memory Warp （Warps 尝试发出内存指令）一个用于计算 warps（尝试发出其他类型的指令的 warps）。在每个队列，warp 都按照 Greedy-then-Oldest 的顺序进行调度。内存相关指令的跟踪是通过增强记分板来指示输出寄存器何时根据负载填充。当观察到工作负载均衡且内存系统未超额订阅时，调度程序在 EP 模式下运行。在 EP 模式下，调度mechanism 首先优先考虑 Memory Warps。由于内存系统没有超额订阅，因此预测尽早启动内存访问将提高性能。在MAP 模式下，调度器会优先考虑计算 warps，以更好地将可用计算与瓶颈内存系统。只有一个内存 warp，即 “owner warp” 被允许发出内存指令，直到它到达依赖于挂起内存的作请求。

除了调度机制之外，Sethia et al. [2015] 表明内存密集型内核的性能比计算密集型内核的峰值 IPC 低得多。他们
说明在内存密集型应用程序中，很大一部分周期都花费在SIMT 内核的负载存储单元由于内存过多导致的内存背压而停止
访问。当 LSU 停止时，有很大一部分时间数据处于就绪状态warps 位于 L1 数据缓存中，但由于 LSU 已备份，因此无法发出 warpwith memory 请求其他 warps。缓存访问重新执行 （CAR） 机制旨在通过在 LSU 管道的一侧提供缓冲区来纠正此行为，该缓冲区将memory 指令并允许其他指令发送到 LSU。请求从re-execution queue （仅当 LSU 未停止且没有新的请求要发出时），除非re-execution queue 已满，在这种情况下，reexecution queue 中的访问将优先处理，直到
释放队列中的空间。

当重新执行队列与内存感知调度器结合使用时，请特别注意需要采取，因为重新执行队列中的请求可能来自 Warps 而不是
优先的 Owner Warp。在 MAP 模式下运行时，非所有者 warp 的请求已发送当它们未在 L1 中命中时，从重新执行队列到 L1 会进一步延迟。在特别是，当来自非所有者 warp 的请求在 L1 中未命中时，该请求不会被中继添加到 L2 缓存中，而是重新插入到重新执行队列的尾部。

## 5.1.3 多核调度研究

**支持 GPU 上的抢占式多任务处理**：Park 等人 [2015] 应对了在 GPU 上支持抢占式多任务处理的挑战。它采用了更宽松的无幂等性定义来启用线程块的上下文切换。他们的提案 Chimera 动态选择三种方法中的一种来实现每个线程块的上下文切换：

- 完全线程块上下文的完整保存/恢复；
- 等待线程块完成；
- 如果由于无幂等性，线程块可以安全地从头开始重新启动，则简单地停止线程块而不保存任何上下文。

对于 Chimera 的每个上下文切换技术，提供了不同的延迟与系统吞吐量影响之间的权衡。该算法估计当前运行的线程块的子集，这些线程块可以以最小的吞吐量影响被停止，以满足用户指定的上下文切换延迟目标。

## 5.1.4 细粒度同步感知调度

ElTantawy 和 Aamodt [2018] 探讨了在运行涉及细粒度同步的代码时，线程束调度的影响。他们使用真实的 GPU 硬件证明，当线程因等待锁而自旋时，会产生显著的开销。他们指出，简单地暂停包含未能获取锁的线程的线程束的执行，可能会阻碍或减缓同一线程束中已持有锁的其他线程的进度。他们提出了一种硬件结构，用于动态识别涉及自旋锁的循环，由于使用了基于栈的重新收敛[ElTantawy 和 Aamodt，2016]，这使得识别工作更具挑战性。这
该结构使用包含程序计数器最低有效位的路径历史记录以及单独的谓词寄存器更新历史记录，以准确检测在锁上循环的循环。为了减少竞争并提高性能，他们提出当在 warp 中持有锁的任何线程释放这些锁之后，执行循环回退分支时，降低被识别为执行循环等待的 warp 的优先级。他们发现，与 Lee 和 Wu [2014] 相比，这分别提高了性能并降低了 1.5 倍和 1.6 倍的能耗。
