# 4.4 GPU 内存系统的未来研究方向

## 4.4.1 内存访问调度与片上互连网络设计

Yuan 等人 [2009] 探讨了针对运行在 GPU 上的 GPGPU 应用程序的内存访问调度器设计。他们观察到，由单个流式多处理器（SM）产生的请求具有行缓冲区局部性。如果一个内存请求序列中的请求在空间上彼此接近，并且访问了同一个 DRAM 银行中的同一个行缓冲区，那么这些请求就被认为具有行缓冲区局部性。然而，当来自一个 SM 的内存请求被发送到同一个内存分区时，它们会与其他 SM 发送到同一内存分区的请求混合在一起。结果是，进入内存分区的请求序列的行缓冲区局部性降低了。为了降低内存访问调度的复杂性，Yuan 等人 [2009] 提出通过修改互连网络来保持行缓冲区局部性。他们引入了仲裁策略，优先授予来自同一个 SM 或具有相似行-银行地址的内存请求包。

Bakhoda 等人 [2010, 2013] 探讨了 GPU 的片上互连网络设计。这种互连网络连接了流式多处理器和内存分区。他们认为，随着 SM 数量的增加，采用可扩展拓扑结构（如网格）将变得必要。他们探讨了网络设计如何影响系统吞吐量，并发现许多 CUDA 应用程序的吞吐量对互连延迟相对不敏感。他们分析了互连流量，并发现其具有多对少对多的流量模式。他们提出了一种更受限的可扩展拓扑结构，由“半路由器”组成，通过利用这种流量模式来降低路由器的面积成本。

## 4.4.2 缓存有效性

Bakhoda 等人 [2009] 研究了在 CUDA 启用的 GPU 上为全局内存访问添加 L1 和/或 L2 缓存的影响，并使用他们的 GPGPU-Sim 模拟器进行了模拟。他们发现，一些应用程序受益，而另一些则没有。

随后，Jia 等人 [2012] 通过对 NVIDIA Fermi GPU 硬件启用或禁用缓存进行了表征研究，并发现了类似的结果。他们观察到，将数据读入共享内存（通过 L1 缓存）的应用程序不会从启用 L1 缓存中受益。即使排除了此类应用程序，Jia 等人 [2012] 也发现，仅靠缓存命中率无法预测缓存是否能够提升性能。他们发现，相反，需要考虑缓存对 L2 缓存（例如，内存分区）请求流量的影响。在他们研究的 Fermi GPU 上，L1 缓存是非分块的，因此，启用缓存可能会导致更大（128 字节）的片外内存访问在缓存未命中时发生。在内存带宽受限的应用程序中，这种额外的片外内存流量可能会导致性能下降。Jia 等人 [2012] 引入了一种基于编译时算法的分类法，使用这种分类法来帮助推断何时为单个加载指令启用缓存是有益的。

## 4.4.3 内存请求优先级与缓存旁路

继上述表征研究 [Jia et al., 2012] 和 Rogers 等人 [2012] 的工作（该工作展示了线程调度可以改善缓存有效性，相关内容在第 5.1.2 节中描述）之后，Jia 等人 [2014] 提出了针对 GPU 的内存请求优先级和缓存旁路技术。缓存的低关联性相对于线程数量而言，可能会导致显著的冲突未命中 [Chen and Aamodt, 2009]。Jia 等人 [2014] 指出，许多 GPGPU 应用程序（用 CUDA 编写）包含数组索引，这些索引会导致单个 warp 的内存请求在使用标准模运算缓存集索引函数 [Hennessy and Patterson, 2011, Patterson and Hennessy, 2013] 时映射到同一个缓存集并发生未命中。

Jia 等人 [2014] 称这种缓存内容争用为 warp 内部争用。假设在检测到未命中时会分配缓存空间，并且有限数量的未命中状态保持寄存器1，warp 内部争用可能导致内存流水线停顿。为了应对 warp 内部争用，Jia 等人 [2014] 提出在发生未命中且由于关联性停顿而无法分配缓存块时，旁路 L1 数据缓存。当缓存集中的所有块都被保留以容纳由未决缓存未命中提供的数据时，就会发生关联性停顿。

Jia 等人 [2014] 还研究了他们所称的跨 warp 争用。这种形式的缓存争用是当一个 warp 赶走另一个 warp 带来的数据时发生的。为了应对这种形式的争用，Jia 等人 [2014] 建议采用他们称之为“内存请求优先级缓冲区”（MRPB）的结构。与 CCWS [Rogers et al., 2012] 类似，MRPB 通过修改对缓存的访问顺序来减少容量未命中，从而增加局部性。然而，与 CCWS 通过线程调度间接实现这一点不同，MRPB 通过改变线程调度后单个内存访问的顺序来增加局部性。

MRPB 在第一级数据缓存之前实现内存请求重新排序。MRPB 的输入是来自指令发布流水线阶段的内存请求（在内存请求合并之后）。MRPB 的输出将内存请求送入第一级缓存。在内部，MRPB 包含几个并行的先进先出（FIFO）队列。使用“签名”将缓存请求分配到这些 FIFO 中。在他们评估的几种选项中，他们发现最有效的签名是使用“warp ID”（介于 0 到流式多处理器上可以运行的最大 warp 数量之间的数字）。MRPB 采用“排水策略”来决定接下来从哪个 FIFO 中选择内存请求以访问缓存。在他们探索的几种选项中，最佳版本是一个简单的固定优先级方案，其中每个队列被分配一个静态优先级，并且最高优先级的队列（包含请求）将首先被服务。

详细的评估表明，结合旁路和使用 MRPB 进行重新排序的机制，与 64 路 16 KB 的缓存相比，可以实现 4% 的几何平均加速比。Jia 等人 [2014] 还与 CCWS 进行了一些比较，显示出更大的改进。我们注意到，Rogers 等人 [2012] 的评估采用了更复杂的集索引哈希函数4来减少关联性停顿的影响。此外，后续工作 [Nugteren et al., 2014] 试图逆向工程 NVIDIA Fermi 架构中实际使用的集索引哈希函数，并发现它使用了异或运算（这也倾向于减少此类冲突）。

类似地，Arunkumar 等人 [2016] 探讨了基于静态指令的内存旁路和缓存行大小变化的影响。他们使用观察到的重用距离模式和内存发散程度来预测旁路和最佳缓存行大小。

Lee 和 Wu [2016] 提出了一种基于控制环路的缓存旁路方法，该方法尝试在运行时基于指令逐条预测重用行为。监控缓存行的重用行为。如果由特定程序计数器加载的缓存行没有足够的重用，那么对该指令的访问将被旁路。

## 4.4.4 利用 warp 间异构性

Ausavarungnirun 等人 [2015] 提出了一系列改进措施，针对 GPU 的共享 L2 和内存控制器，以缓解不规则 GPU 应用程序中的内存延迟发散问题。这些技术，统称为内存发散校正（MeDiC），利用了 warp 间在内存延迟发散程度上的异构性这一观察结果。根据它们与共享 L2 缓存的交互方式，内核中的每个 warp 可以被归类为全/大部分命中、全/大部分未命中或平衡。作者们展示了对于非全命中的 warp 来说，几乎没有收益，因为全命中的 warp 必须等待最慢的访问返回后才能继续前进。他们还展示了 L2 缓存的排队延迟对性能的影响不容小觑，并且这种影响可以通过为全命中 warp 的所有请求（即使是可能命中的请求）旁路 L2 缓存来最小化。这通过减少排队延迟，降低了全命中 warp 的访问延迟。除了自适应旁路技术外，他们还提出了对缓存替换策略和内存控制器调度器的修改，试图最小化被检测为全命中 warp 的延迟。

作者们提出的微架构机制包括四个组成部分：（1）一个 warp 类型检测块——它将 GPU 中的 warp 分类为五种潜在类型之一：全未命中、大部分未命中、平衡、大部分命中或全命中；（2）一个 warp 类型感知旁路逻辑块，用于决定请求是否应该旁路 L2 缓存；（3）一个 warp 类型感知插入策略，用于确定在 LRU 栈中放置 L2 插入的位置；（4）一个 warp 类型感知内存调度器，用于确定如何将 L2 未命中/旁路发送到 DRAM。

检测机制通过在基于间隔的基础上采样每个 warp 的命中率（总命中数/总访问次数）来运行。根据这个比率，warp 被归类为上述五种分类之一。用于这些分类的确切命中率边界会根据每个工作负载动态调整。在分类间隔期间，不会请求旁路缓存，以便对 warp 的 L2 特性变化做出反应。

旁路机制位于 L2 缓存之前，并接收带有生成它们的 warp 类型标记的内存请求。这个机制试图消除来自全未命中 warp 的访问，并将大部分未命中 warp 转化为全未命中 warp。该块简单地将所有标记为来自全未命中和大部分未命中 warp 的请求直接发送到内存调度器。

缓存管理策略 MeDiC 通过改变从 DRAM 返回的请求在 L2 的 LRU 栈中的放置位置来运行。来自大部分未命中 warp 的缓存行请求被插入到 LRU 位置，而所有其他请求都被插入到传统的 MRU 位置。

最后，MeDiC 修改了基线内存请求调度器，使其包含两个内存访问队列：一个用于全命中和大部分命中的 warp 的高优先级队列，以及一个用于平衡、大部分未命中和全未命中的 warp 的低优先级队列。内存调度器简单地优先处理高优先级队列中的所有请求，而不是低优先级队列中的任何请求。

## 4.4.5 协调缓存旁路

Xie 等人 [2015] 探索了通过选择性启用缓存旁路来提高缓存命中率的潜力。他们采用剖析来确定 GPGPU 应用程序中每个静态加载指令是否具有良好的局部性、较差的局部性或适中的局部性。他们相应地标记这些指令。标记为具有良好局部性的加载操作被允许使用 L1 数据缓存。标记为具有较差局部性的加载操作始终被旁路。标记为具有适中局部性的加载指令采用自适应机制，该机制的工作方式如下。自适应机制以线程块粒度运行。对于给定的线程块，所有执行的适中局部性加载操作都被统一处理。行为是在线程块启动时确定的，基于在线调整的阈值，该阈值使用考虑 L1 缓存命中和流水线资源冲突的性能指标。他们的评估表明，这种方法比静态 warp 限制显著提高了缓存命中率。

## 4.4.6 自适应缓存管理

Chen 等人 [2014b] 提出了协调缓存旁路和 warp 节流，利用 warp 节流和缓存旁路两者的优势，以提高对高度缓存敏感的应用程序的性能。该机制在运行时检测缓存争用和内存资源争用，并相应地协调节流和旁路策略。该机制通过现有的 CPU 缓存旁路技术实现缓存旁路，即保护距离，防止缓存行在一定数量的访问后被驱逐。在插入缓存时，缓存行被分配一个保护距离，并且计数器跟踪剩余的保护距离。一旦剩余保护距离达到 0，该行就不再受到保护，可以被驱逐。当新的内存请求试图将新行插入到没有未受保护行的集中时，内存请求将旁路缓存。

保护距离是全局设置的，不同工作负载的最优值各不相同。在这项工作中，Chen 等人 [2014b] 扫描了静态保护距离，并证明 GPU 工作负载对保护距离值相对不敏感。

## 4.4.7 缓存优先级

Li 等人 [2015] 观察到，warp 节流优化了 L1 缓存命中率，同时可能使其他资源（如片外带宽和 L2 缓存）显著未充分利用。他们提出了一种机制，通过为 warp 分配令牌来决定哪些 warp 可以将行分配到 L1 缓存中。额外的“非污染性”warp 没有被赋予令牌，因此尽管它们可以执行，但不允许它们驱逐 L1 中的数据。这导致了一个优化空间，其中可以设置调度的 warp 数量（W）和拥有令牌的 warp 数量（T），这两个数量都可以小于可以执行的最大 warp 数量。他们展示了通过静态选择 W 和 T 的最优值，可以比 CCWS 静态 warp 限制提高 17% 的性能。

基于这一观察，Li 等人 [2015] 探索了两种机制，以学习 W 和 T 的最佳值。第一种方法基于在保持高线程级并行性的同时增加缓存命中率的想法。在这种方法中，称为 dynPCALMTLP，采样周期以最大 warp 数量运行内核，然后在不同的 SIMT 核心中变化 T。选择实现最大性能的 T 值。这实现了与 CCWS 相比具有显著较少面积开销的可比性能。第二种方法，称为 dynPCALCCWS，最初使用 CCWS 设置 W，然后使用 dynPCALMTLP 确定 T。然后，它监控共享结构的资源使用情况，以动态增加或减少 W。这比 CCWS 实现了 11% 的性能提升。

## 4.4.8 虚拟内存页面放置

Agarwal 等人 [2015] 考虑了在包括容量优化和带宽优化内存的异构系统中支持跨多个物理内存类型缓存一致性的影响。由于带宽优化的 DRAM 比容量优化的 DRAM 更昂贵，未来系统可能会同时包含这两种内存。Agarwal 等人 [2015] 观察到，当前操作系统页面放置策略（如 Linux 中部署的策略）没有考虑到内存带宽的非均匀性。他们研究了一个未来系统，其中 GPU 可以访问低带宽/高容量的 CPU 内存，延迟较低——延迟为 100 个核心周期。他们的实验使用了修改版的 GPGPU-Sim 3.2.2，配置了额外的 MSHR 资源，以模拟更近期的 GPU。

在这种设置下，他们首先发现，对于带宽受限的内存应用程序，通过使用 CPU 和 GPU 内存来增加聚合内存带宽，存在显著的性能提升机会。他们发现对于内存延迟受限的 GPGPU 应用程序，情况并非如此。在假设页面访问均匀且带宽优化内存的容量不是限制因素的情况下，他们展示了一个简单的策略，即根据区域的可用内存带宽按比例分配页面，这是最优的。假设带宽优化内存的容量不是问题，他们发现一个简单的策略，即随机将页面分配到带宽或容量优化的内存中，分配概率与内存带宽成比例，在实际的 GPGPU 程序中可以工作得很好。然而，当带宽优化内存的容量不足以满足应用程序需求时，他们发现需要细化页面放置，以考虑访问频率。

为了细化页面放置，他们提出了一个涉及剖析的系统，该系统使用修改版的 NVIDIA 开发工具 nvcc 和 ptxas，以及对现有 CUDA API 的扩展，以包括页面放置提示。使用剖析引导的页面放置提示可以获得约 90% 的最佳页面放置算法的好处。他们将页面迁移策略留作未来工作。

## 4.4.9 数据放置

Chen 等人 [2014a] 提出了 PORPLE，这是一种可移植的数据放置策略，包括规范语言、源到源编译器和自适应运行时数据放置器。他们利用了这样一个观察结果：由于 GPU 上有各种各样的内存，确定数据应该放置在哪里对于程序员来说是困难的，并且通常不具有从一个 GPU 架构到下一个架构的可移植性。PORPLE 的目标是具有可扩展性、输入自适应性，并且适用于规则和不规则数据访问。他们的方法依赖于三个解决方案。

第一个解决方案是内存规范语言，以帮助实现可扩展性和可移植性。内存规范语言根据访问这些空间的条件描述 GPU 上的所有各种内存。例如，对相邻全局数据的访问是合并的，因此是并发访问的，但对共享内存同一银行的访问必须是串行化的。

第二个解决方案是名为 PORPLE-C 的源到源编译器，它将原始 GPU 程序转换为与放置无关的版本。编译器在内存访问周围插入保护，选择对应于预测的最佳数据放置的访问。

最后，为了预测哪种数据放置最优化，他们使用 PORPLE-C 通过代码分析找到静态访问模式。当静态分析无法确定访问模式时，编译器生成一个函数，该函数跟踪运行时访问模式并尝试进行预测。这个函数在 CPU 上运行很短时间，有助于在启动内核之前确定最佳的 GPU 数据放置。在这项工作中，该系统只处理数组的放置，因为数组是 GPU 内核中最常用的数据结构。

用于进行放置预测的轻量级模型在 PORPLE 中生成基于内存访问的序列化条件的事务数量估计。对于具有缓存层次结构的内存，它使用基于重用距离的缓存命中率估计。当多个数组共享一个缓存时，基于数组大小对缓存的分配估计是基于线性划分的。

## 4.4.10 多芯片模块 GPU

Arunkumar 等人 [2017] 指出，随着摩尔定律的放缓，GPU 性能的提升将放缓。他们提出通过构建大型 GPU，由小型 GPU 模块组成多芯片模块（见图 4.4）来扩展性能提升。他们展示了通过结合本地缓存远程数据、考虑局部性的 CTA 调度到模块以及首次触碰页面分配，可以实现单个大型（且不可实现的）单片 GPU 性能的 10%，比使用相同工艺技术可实现的最大单片 GPU 性能高出 45%。
