### 第3章 内存层次结构

NVIDIA GPU架构随着新版本的推出变得越来越复杂。随着架构的演变，深入理解GPU的内存层次结构对于编写高效代码至关重要。

为了让设计者能够将工作集最优地映射到内存层次结构上，了解每个缓存级别的大小、该内存是否与其他缓存共存（可能导致内容被逐出）、以及每个缓存内存是否私有于某个流式多处理器（SM）或者被所有SM共享，显得尤为重要。

在本章中，我们将详细描述Turing的内存层次结构（见图3.1）。具体来说，我们将揭示：

- 所有缓存级别和地址转换后备缓冲区（TLBs）的几何结构、属性和性能；
- 寄存器文件银行及其冲突情况；
- 在负载下共享内存和全局内存的性能。

表3.1总结了我们的发现，并将Turing与Volta、Pascal、Maxwell和Kepler代进行了比较。

T4 GPU采用了GDDR6内存，其带宽为320 GB/s（内存时钟频率为5001 MHz），并配备了4096 KiB的L2缓存[3]。从全局内存加载的数据会隐式地缓存在L1和L2中。

### 3.1 L1数据缓存

Turing采用了与Volta相同的L1数据缓存/共享内存组合设计。这种设计降低了缓存命中延迟，并提高了与Pascal架构相比的带宽。

关于L1数据缓存的几何结构，我们的发现与Turing和Volta架构白皮书[10, 3]中所报告的内容一致。具体来说，T4提供了两倍于P4 GPU的L1数据容量和两倍的带宽。

在性能方面，我们的实验表明，在T4 GPU上，L1数据缓存的带宽大约是其P4前身的3.7倍。

#### 3.1.1 延迟和带宽

我们在T4 GPU上测量到的L1数据缓存命中延迟为32个时钟周期，相比之下，P4为82个时钟周期（见图3.5）。

在Turing和Volta之前，Kepler是最近一个将L1缓存和共享内存组合在一起的架构。Kepler的L1缓存读命中延迟为35个时钟周期。尽管T4的时钟频率几乎是K80的两倍（1590 MHz对比875 MHz），但Turing的L1延迟仍然优于Kepler。

我们使用以下基准测试来测量T4 GPU的L1数据缓存加载带宽。该基准测试扫描一个包含32位元素的数组；每个线程块访问数组中的所有元素：

```c
__global__ void l1_bw(
    uint32_t *startClk, uint32_t *stopClk,
    float *dsink, uint32_t *posArray
) {
    // 线程索引
    uint32_t tid = threadIdx.x;
    // 用于防止编译器优化掉代码的副作用变量
    float sink = 0;
    // 通过填充来预热L1缓存
    for (uint32_t i = tid; i < L1_SIZE; i += THREADS_NUM) {
        float *ptr = posArray + i;
        asm volatile ("{\t\n"
            ".reg .f32 data;\n\t"
            "ld.global.ca.f32 data, [%1];\n\t"
            "add.f32 %0, data, %0;\n\t"
            "}"
            : "+f"(sink)
            : "l"(ptr)
            : "memory");
    }
    // 同步所有线程
    asm volatile ("bar.sync 0;");
    // 开始计时
    uint32_t start = 0;
    asm volatile ("mov.u32 %0, %%clock;" : "=r"(start) :: "memory");
    // 从L1缓存加载数据并累加
    for (uint32_t i = 0; i < L1_SIZE; i += THREADS_NUM) {
        float *ptr = posArray + i;
        // 每个线程块加载L1缓存中的所有数据
        for (uint32_t j = 0; j < THREADS_NUM; j += WARP_SIZE) {
            uint32_t offset = (tid + j) % THREADS_NUM;
            asm volatile ("{\t\n"
                ".reg .f32 data;\n\t"
                "ld.global.ca.f32 data, [%1];\n\t"
                "add.f64 %0, data, %0;\n\t"
                "}"
                : "+f"(sink)
                : "l"(ptr + offset)
                : "memory");
        }
    }
    // 同步所有线程
    asm volatile ("bar.sync 0;");
    // 停止计时
    uint32_t stop = 0;
    asm volatile ("mov.u32 %0, %%clock;" : "=r"(stop) :: "memory");
    // 写入时间
    startClk[tid] = start;
    stopClk[tid] = stop;
    dsink[tid] = sink;
}
```

我们在表3.2中报告了在各种GPU设备上测量到的L1数据带宽，以及它们的理论上限。

我们在T4 GPU上测量到的实际带宽为58.83字节/时钟周期/SM，即P4 GPU（15.7字节/时钟周期/SM）的3.7倍。由于T4和P4卡的图形时钟频率非常接近（fg），因此以时钟周期为单位的带宽比较是有意义的。

我们通过将每个SM的加载/存储单元（LSU）数量乘以每个LSU每条指令每时钟周期可以加载的字节数来计算理论吞吐量。

历史上，采用L1缓存与共享内存组合的架构（Turing、Volta和Kepler）比L1缓存和共享内存分开的架构（Pascal和Maxwell）具有更高的L1带宽。

#### 3.1.2 几何结构

根据Turing白皮书[3]，加载/存储操作可以使用32 KiB或64 KiB的L1数据缓存。

基于Mei和Chu的细粒度指针追逐技术[9]的实验，我们无法检测到配置的完整大小，并且在Volta和Turing架构上都比名义L1数据缓存大小少了7 KiB（见表3.3）。

在我们的实验设置中，共享内存被配置为64 KiB的大小。然后我们使用一个基准测试，该测试扫描一个长度可变的数组A两次。只要A的大小超过25 KiB，我们就能检测到缓存未命中。

目前我们无法解释这7 KiB的差异。我们推测这是由于新应用的替换策略造成的。我们确认这与ECC（错误校正）功能无关。

表3.1描述了我们发现的Turing的L1数据缓存几何结构的其余部分。Turing的L1数据缓存的行大小、加载和更新粒度与Volta、Pascal和Maxwell GPU相同。

在我们之前关于Volta的报告[2]中，我们发现Volta的L1缓存替换策略相对于其前身有所改进。Turing也采用了旨在保护大数组免受稀疏内存访问引起的逐出的L1缓存替换策略。

我们使用一个扫描可变长度数组两次的基准测试，并在L1缓存饱和时记录位置和延迟数据。我们发现，当L1数据缓存饱和时，Turing会随机逐出4个连续的缓存行（128字节）。我们观察到，一旦一组缓存行被逐出，第二次扫描将导致同一组中更多的缓存行被逐出。
