4 Instruction latency and throughput

4.1 Native instructions

4.2 Atomic operations


### 第4章 指令延迟与吞吐量

在本章中，我们报告了Turing架构中本地指令的延迟情况。我们还对原子操作的性能进行了基准测试，并将其与旧设备进行了比较。我们评估了T4 GPU在单精度、双精度和半精度浮点运算中的性能，并评估了更新后的Tensor Cores。

#### 4.1 本地指令

Turing和Volta的指令通常比Pascal及更老的GPU架构具有更低的延迟，但Turing在指令延迟方面似乎并没有比Volta有改进。在本节中，我们在表4.1中报告了Turing、Volta和Pascal上常用指令的延迟。

正如Turing白皮书[3]所提到的，核心FMA数学运算的依赖性延迟为4个时钟周期，这与Volta相同。

在Turing上，我们发现大多数整数、单精度和半精度指令的延迟与Volta上的相同，而双精度指令的延迟则超过了40个周期。

在Volta上，大多数整数和单精度指令的延迟为4个周期。在我们之前的研究中，我们确定了Volta上大多数双精度指令的延迟为8个周期，而半精度指令的延迟为6个周期。

在Maxwell和Pascal上，IMAD和IMUL指令的延迟较长，因为它们是通过软件模拟实现的。

在Pascal上，大多数整数和单精度指令的延迟为6个周期；双精度指令的延迟为8个周期；更复杂的指令（其中一些在SFU上运行）需要14个周期。

实验设置：在软件调度的GPU上测量依赖性指令延迟需要使用定制的基准测试，这些基准测试的设计如下。

为了测量指令A的延迟，我们添加了一个依赖于A的第二个指令B，然后设置了调节A执行的控制字：

- 如果A具有固定的延迟，我们选择一个消耗A输出的B。我们减少A的控制字中的停顿周期，直到A的结果被B消耗时出现错误。最后一个产生正确结果的停顿值即为A的延迟；
- 如果A具有可变延迟，我们选择一个已知延迟的B，然后设置控制标志以在A和B之间创建一个人为的读/写依赖关系。我们让调度器等待这个依赖关系，然后使用一对CS2R指令测量这对指令的累积延迟，并通过减去已知的B的延迟来获得A的延迟。

#### 4.2 原子操作

我们的测量表明，与Volta相比，Turing上共享内存中原子操作的延迟略有增加，但比Pascal和更老的架构要短。我们在表4.2中报告了这些以时钟周期为单位的延迟。即使在实际值方面，这种比较也是有意义的，因为不同GPU采用了相似的时钟频率（见表3.1）。

正如全局内存中的原子操作一样，与V100相比，T4设备上的延迟似乎有所增加。M60 GPU在所有考虑的GPU中具有最佳的延迟。

值得注意的是，Kepler是唯一一个共享内存中原子操作比全局内存中原子操作慢的架构，而且慢得多（4到8倍）。这是由于Kepler缺乏对共享内存中原子操作的硬件支持。此外，它的模拟原子操作在竞争激烈时会迅速退化。后续架构在硬件中支持原子操作，并且即使在存在竞争的情况下也能提供低延迟的原子操作。

我们通过以下方式测量这些延迟：确定原子指令A的延迟，然后跟随一个访问同一位置的加载指令B，该指令的延迟已知。我们从这对指令（A，B）的累积延迟中减去B的已知延迟，从而得出A的延迟。

图4.1报告了从Kepler到Turing的GPU在存在竞争的情况下测量到的吞吐量，分为四种场景：

- **场景1**：一个包含1024个线程的线程块。其中，R个线程访问同一地址，而其他线程访问全局内存中连续的L2缓存行。8组线程访问同一个L2缓存行；
- **场景2**：一个包含1024个线程的线程块。其中，R个线程访问同一地址，而其他线程访问全局内存中连续的L2缓存行，每组线程访问一个单独的L2缓存行；
- **场景3**：多个包含1024个线程的线程块。所有线程块中的所有线程都访问同一地址；线程块之间存在严重竞争；
- **场景4**：多个包含1024个线程的线程块。每个线程块中的所有线程访问同一地址。不同线程块访问不同的地址；线程块之间没有竞争。

T4 GPU在所有场景中都没有实现最高的吞吐量。唯一T4 GPU表现最佳的场景是在多个SM上且SM之间没有竞争的场景。在所有场景中，从Maxwell到Pascal的聚合吞吐量都有显著提升。
