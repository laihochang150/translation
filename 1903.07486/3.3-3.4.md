3.3 Instruction cache hierarchy

3.4 Constant memory hierarchy




### 3.3 指令缓存层次结构

在本节中，我们通过实验映射了指令缓存层次结构的大小和组织方式。具体来说，包括以下两个方面：

1. **检测每个缓存级别的大小**。
2. **确定缓存级别在GPU架构块（调度器、SM、整个芯片）中的分布**。

#### 3.3.1 分类

所有我们考虑的GPU架构（包括Turing）都具有三个级别的指令缓存。为了避免混淆，注意在Turing和Volta上，这三个级别分别命名为L0、L1和L2，而在之前的架构（Pascal、Maxwell和Kepler）中，它们被命名为L1、L1.5和L2。为了与NVIDIA的白皮书[3, 10]以及先前的研究保持一致，我们采用这种分类方式。请注意，表达“指令缓存的第二级别”时，它在Turing和Volta中指的是L1，而在Pascal、Maxwell和Kepler中指的是L1.5。

#### 3.3.2 大小

为了检测每个缓存级别的大小，我们研究了随着指令序列长度的增加，由长指令序列实现的平均逆吞吐量（即平均每个指令的时钟周期，CPI）的变化。随着我们增加指令序列的长度，我们期望看到一个恒定的CPI值，直到序列超过缓存的大小。实际上，实验结果显示出平台和斜坡（见图3.6），这些分别对应于缓存级别的大小以及从一个级别到下一个级别的转换。在图中，顶部图表关注内存层次结构中的前两个级别，而底部图表关注最后一个级别与全局设备内存之间的转换。

我们在表3.1中报告了所有发现。与之前的架构相比，Turing在访问第二和第三指令缓存级别时表现出更好的逆吞吐量。

实验设置：我们的基准测试测量了给定长度的指令序列的平均CPI，这些指令序列对数据缓存层次结构没有压力。我们从缓存行大小开始，逐步迭代测量，直到达到L3的合理大小。基准测试两次执行每个序列，但只对第二次执行进行计时，以便我们只测量容量未命中和冲突未命中，而不是冷未命中。

- 在Pascal、Maxwell和Kepler上，为了与我们之前的报告[2]保持一致性，我们采用了相同的技术，即长的FFMA指令序列，其寄存器操作数被选择为每个指令与其邻居没有寄存器依赖性。
- 在Volta和Turing上，我们切换到使用NOP序列的更简单方法，而不是FFMA。这种方法避免了在Volta和Turing这两个GPU上，NVCC不期望地在连续的FFMA指令之间生成2周期停顿的情况。

#### 3.3.3 组织

在不同的GPU架构中，指令内存层次结构的级别按以下方式组织：

- 在Turing和Volta上，每个L0指令缓存私有于一个调度器/处理块；
- 在所有考虑的GPU上，每个L1指令缓存私有于一个SM；
- 在Pascal、Maxwell和Kepler上，每个L1.5指令缓存私有于一个SM；在Turing和Volta上不存在L1.5指令缓存；
- 在所有考虑的GPU上，L2缓存是统一的（即，它缓存指令和数据），并且由所有SM共享。

对于旧于Turing的架构，我们在之前的报告[2]中提供了实验支持。对于关于Turing的声明，我们通过以下方式收集了证据。

我们的实验测量了攻击者线程和受害者线程之间的交互。两个线程都循环执行选定长度的NOP指令序列：

- 受害者线程仅运行固定长度的NOP序列，通常设计为适合某个特定指令缓存级别；我们将此称为受害者探测序列；
- 攻击者线程运行除了与受害者相同的探测序列之外，还有一个在它之前的可变长度NOP序列，旨在对给定缓存级别造成压力，可能会逐出指令缓存条目。

我们监控攻击者线程造成的逐出是否只影响它自己的性能，或者是否也影响受害者。如果受害者不受影响，则最小的缓存级别适合固定长度的受害者探测序列，该缓存级别私有于两个线程正在运行的架构块（即GPU处理块或SM）；否则，缓存级别被两个线程共享，并且位于考虑的块之外。在我们的实验中，两个线程通过测量它们的逆吞吐量（CPI）来监控它们的性能。

结果表明，每个L0指令缓存私有于一个处理块，每个L1指令缓存私有于一个SM，而L2缓存由所有SM共享（见图3.7）。

为了检查L0、L1和调度器（或GPU处理块）之间的关系，我们使用了攻击者和受害者线程在同一个SM上运行，但位于不同处理块中的实验。我们在攻击者线程中使用越来越长的序列。为了排除强制性未命中，我们让攻击者和受害者通过分别运行它们各自的序列来预热缓存。

我们观察到：

- 当攻击者序列增长但保持在L0容量范围内时，只有攻击者经历了速度下降（见图3.7左上角），而受害者不受影响。这表明两个线程访问了不同的L0缓存，这些缓存私有于每个处理块；
- 当指令序列增长超过L0容量并进入L1时（见图3.7右上角），两个线程的性能都以类似的方式下降，这表明两个线程共享L1。

接下来，我们通过类似构建的实验检查L1和L2级别以及SM之间的关系。这一次，两个线程在不同的SM上运行（SM0和SM1）。

我们观察到：

- 当攻击者序列超出L0但保持在L1容量范围内时，只有攻击者线程经历了与L1命中率相对应的速度下降（见图3.7左下角）；受害者仍然运行一个适合L0（16 KiB）的固定长度探测序列，不受影响。这表明不同的SM具有不同的L1缓存；
- 当攻击者序列超出L2容量时（见图3.7右下角），受害者和攻击者都经历了速度下降。这表明不同的SM访问同一个L2缓存。


### 3.4 常量内存层次结构

常量内存是全局内存的一个缓存窗口，专门用于存储使用 `__constant__` 关键字声明的数据，以及内核调用参数和立即数常量。我们发现Turing架构具有三个级别的常量缓存内存，其几何结构和性能特性如表3.1所示，延迟如图3.9所示。

Turing的常量内存层次结构与之前几代架构相比没有显著变化。在我们考虑的所有GPU架构中，以下属性始终成立：

- L1常量缓存采用非最近最少使用（LRU）替换策略；
- 每个SM拥有两个私有的常量缓存级别，我们分别将其称为L1和L1.5常量缓存（在同一个SM内访问一个缓存级别的任何一个缓存都不会影响另一个缓存级别）；
- L2缓存是第三级常量缓存。它被所有SM共享，并且是用于指令和数据的统一缓存。

在Turing中，就像在Volta中一样，第二级常量缓存和第二级指令缓存由相同的硬件缓存支持。更具体地说，L1.5常量缓存和L1指令缓存是相同的。为了证明这一说法，我们运行了一个攻击者-受害者实验，其中我们展示了指令序列长度不断增加（攻击者）会逐出预先填充在L1.5常量缓存中的条目。我们通过记录随后执行的常量数组扫描（受害者）的执行时间来检测这些逐出。我们使用由相同FFMA指令组成的指令序列作为攻击者。

实验结果（见图3.8）表明，随着攻击者指令序列长度的增加，受害者遭受的未命中率相应增加。我们观察到受害者的未命中率在0%到100%之间变化。

与之前架构一样，Turing上的常量内存访问支持广播功能（见图3.9）。当一个线程束中的所有线程访问同一个地址时，常量内存会同时将数据发送给所有线程。当线程访问不同的地址时，这些访问会被串行化。
